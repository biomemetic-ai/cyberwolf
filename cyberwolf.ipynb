{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶æ Cyberwolf üê∫\n",
        "\n",
        "**Let's play Werewolf with LLMs!**\n",
        "\n",
        "Werewolf is a social deception game, where the participants are divided into Werewolves and Villages.\n",
        "\n",
        "The villagers' goal is to lynch all the werewolves, while the werewolves' goal is to gain majority in the village.\n",
        "\n",
        "During the day, Werewolves turn human, and try to stay undetected while the village discusses and votes on who to lynch. But during the night, they turn into blood-thirsty monsters! They then decide who to kill, and the next day, the villagers wake up to find one of their own dead.\n",
        "\n",
        "When humans play it, everyone sits in a circle and closes their eyes. The werewolves then wake up (open their eyes) and decide who to kill (by pointing at them) before falling back asleep. Afterward, during the day, everyone wakes up and the game master tells them who was mauled during the night.\n",
        "\n",
        "This is a game that is verbal in nature, and where deception and strategy come into play. So what happens when we get AI to play it against each other? Let's find out!"
      ],
      "metadata": {
        "id": "RpnAlYaAJyeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select the LLM model you want to use. { display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "llm_model_option = 'gpt-4-1106-preview' # @param [\"llama-2-13b-chat\", \"gpt-4-1106-preview\"]\n",
        "\n",
        "local_models = ['llama-2-13b-chat']\n",
        "\n",
        "model_is_local = llm_model_option in local_models\n",
        "\n",
        "if model_is_local:\n",
        "    os.environ[\"MODEL_IS_LOCAL\"] = \"1\"\n",
        "else:\n",
        "    os.environ[\"MODEL_IS_LOCAL\"] = \"0\""
      ],
      "metadata": {
        "id": "csVDsbt0_dvP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If using Llama - Set up the GPU\n",
        "\n",
        "To start off, let's install and configure a large language model. This example runs in the free version of Google Colab with a T4 GPU.\n",
        "\n",
        "We will use llama-index to power the model and llama2 as our model.\n",
        "\n",
        "We first download llama-index via pip (it doesn't come pre-installed)."
      ],
      "metadata": {
        "id": "pAqPZkESK-Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if [ \"$MODEL_IS_LOCAL\" = \"1\" ]\n",
        "then\n",
        "    pip install llama-index wurlitzer\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJkcSCCWfOXJ",
        "outputId": "c71b4d0a-b225-450d-d665-5692ac1c5802",
        "ExecuteTime": {
          "end_time": "2023-11-16T19:47:08.248142Z",
          "start_time": "2023-11-16T19:46:25.398920Z"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then compile it to use CUDA (make sure that you use a T4 GPU). You will need about 12GB of VRAM.\n",
        "\n",
        "This step might take a minute."
      ],
      "metadata": {
        "id": "MjZmE1uULeMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if [ \"$MODEL_IS_LOCAL\" = \"1\" ]\n",
        "then\n",
        "    CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_fo4RLylGZF",
        "outputId": "4c1ab458-36de-43e4-bf87-9ceffcec7073",
        "ExecuteTime": {
          "end_time": "2023-11-16T19:47:08.254592Z",
          "start_time": "2023-11-16T19:47:08.253632Z"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will download a LLAMA-2 model directly from HuggingFace."
      ],
      "metadata": {
        "id": "N_TOin1aLwKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_is_local:\n",
        "    model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\""
      ],
      "metadata": {
        "id": "XIdmbTd1kPSO",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.254011Z"
        }
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_is_local:\n",
        "    from llama_index import (\n",
        "        SimpleDirectoryReader,\n",
        "        VectorStoreIndex,\n",
        "        ServiceContext,\n",
        "    )\n",
        "    from llama_index.llms import LlamaCPP\n",
        "    from llama_index.llms.llama_utils import (\n",
        "        messages_to_prompt,\n",
        "        completion_to_prompt,\n",
        "    )"
      ],
      "metadata": {
        "id": "6tE4V6_fkUgA",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.254120Z"
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we continue, we will also install wurlitzer. This is because we want to be able to capture C++'s stderr sys pipe when we load LlamaCPP - otherwise we cannot see if the runner picks up CUDA!"
      ],
      "metadata": {
        "id": "-kdbod6vL4bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_is_local:\n",
        "    from wurlitzer import pipes, sys_pipes"
      ],
      "metadata": {
        "id": "3xVOMhQs7E7a",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.256681Z"
        }
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the model. It is important that you transfer all model layers to the GPU (`n_gpu_layers` below) - otherwise, each steps takes minutes to compute.\n",
        "\n",
        "You should also see something like\n",
        "\n",
        "```\n",
        "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
        "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
        "ggml_init_cublas: found 1 CUDA devices:\n",
        "```\n",
        "\n",
        "in the logs. This confirms that you are now running on CUDA.\n",
        "\n",
        "Note - you cannot set `verbose=False` right now - see bug https://github.com/abetlen/llama-cpp-python/issues/729"
      ],
      "metadata": {
        "id": "BkQ1NUOMMMnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_is_local:\n",
        "    with sys_pipes():\n",
        "        llm = LlamaCPP(\n",
        "            # You can pass in the URL to a GGML model to download it automatically\n",
        "            model_url=model_url,\n",
        "            # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "            model_path=None,\n",
        "            temperature=0.1,\n",
        "            max_new_tokens=256,\n",
        "            # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "            context_window=3900,\n",
        "            # kwargs to pass to __call__()\n",
        "            generate_kwargs={},\n",
        "            # kwargs to pass to __init__()\n",
        "            # set to at least 1 to use GPU\n",
        "            model_kwargs={\"n_gpu_layers\": 43},\n",
        "            # transform inputs into Llama2 format\n",
        "            messages_to_prompt=messages_to_prompt,\n",
        "            completion_to_prompt=completion_to_prompt,\n",
        "            verbose=True,\n",
        "      )"
      ],
      "metadata": {
        "id": "_FYnaNGykHzo",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.258094Z"
        }
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_is_local:\n",
        "    def call_llm(instructions: str, prompt: str):\n",
        "        response = llm.chat(\n",
        "            messages=[\n",
        "                ChatMessage(\n",
        "                    role=MessageRole.SYSTEM,\n",
        "                    content=instructions\n",
        "                ),\n",
        "                ChatMessage(\n",
        "                    role=MessageRole.USER,\n",
        "                    content=prompt\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        return response.message.content.strip()"
      ],
      "metadata": {
        "id": "reAedgIjG_Sv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If using OpenAI - enter your API token"
      ],
      "metadata": {
        "id": "H_KxJDh6A6Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if [ \"$MODEL_IS_LOCAL\" = \"0\" ]\n",
        "then\n",
        "    pip install openai python-dotenv\n",
        "fi"
      ],
      "metadata": {
        "id": "HsW2qGaEDnAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8b3cd7-4671-4d1c-e02c-e46b24517797"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not model_is_local:\n",
        "    from getpass import getpass\n",
        "    from dotenv import load_dotenv\n",
        "    import openai\n",
        "\n",
        "    openai_api_key = getpass('Enter the OpenAI API Key: ')\n",
        "\n",
        "    with open('.env', 'w') as f:\n",
        "        f.write(f'OPENAI_API_KEY={openai_api_key}')\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "    def call_llm(instructions: str, prompt: str):\n",
        "        response = openai.chat.completions.create(\n",
        "            model=llm_model_option,\n",
        "            max_tokens=50,\n",
        "            messages=[\n",
        "                ChatMessage(\n",
        "                    role=MessageRole.SYSTEM,\n",
        "                    content=instructions\n",
        "                ),\n",
        "                ChatMessage(\n",
        "                    role=MessageRole.USER,\n",
        "                    content=prompt\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        response = response.choices[0].message.content.lstrip()\n",
        "        return response"
      ],
      "metadata": {
        "id": "P5R-gFbEBFnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a41f38-0aa8-4c88-e4af-56d05f574164"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's start playing"
      ],
      "metadata": {
        "id": "-YYVb0wwJQVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the game"
      ],
      "metadata": {
        "id": "54ch1tCeAY-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dependencies\n",
        "\n",
        "First, let us import a number of things that we will use later!"
      ],
      "metadata": {
        "id": "qMhVO8-AMjj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "from pydantic import BaseModel\n",
        "from typing import *\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import sys\n",
        "from llama_index.llms.base import ChatMessage, MessageRole"
      ],
      "metadata": {
        "id": "mh7hwDz5BkAK",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.259628Z"
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up helpers"
      ],
      "metadata": {
        "id": "40BRrSYqM7da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us define two helper functions. They each take instructions and a prompt, and then return a response. The response is either a choice (for voting) or a free text reponse, for the other players."
      ],
      "metadata": {
        "id": "MScE1HeuM5cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose(instructions: str, prompt: str, choices: list[str]):\n",
        "    formatted_choices = '\\n'.join(f'  - {choice}' for choice in choices)\n",
        "    instruction_stem = f\"\"\"\n",
        "RESPOND ONLY WITH ONE OF THE FOLLOWING CHOICES:\n",
        "{formatted_choices}\n",
        "\"\"\"\n",
        "    instructions += instruction_stem\n",
        "    prompt += instruction_stem\n",
        "\n",
        "    response_choice = call_llm(instructions, prompt)\n",
        "    if response_choice not in choices:\n",
        "        for choice in choices:\n",
        "            if choice in response_choice:\n",
        "                response_choice = choice\n",
        "                break\n",
        "        else:\n",
        "            print(\"Invalid choice:\", response_choice)\n",
        "            return None\n",
        "    return response_choice"
      ],
      "metadata": {
        "id": "T4OUT8juFvdU",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.261287Z"
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_with_instructions(instructions: str, prompt: str):\n",
        "    response = call_llm(instructions, prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "EEoAbvaVGQ8h",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.262943Z"
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the game\n",
        "\n",
        "In here we will define the roles, names of players and other aspects.\n",
        "\n",
        "We choose to keep it simple, with villages, werewolves and seers.\n",
        "\n",
        "Feel free to configure the number of players, roles and other aspects of the game here."
      ],
      "metadata": {
        "id": "QVihPRxmNQ4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VILLAGER = \"Villager\"\n",
        "WEREWOLF = \"Werewolf\"\n",
        "SEER = \"Seer\"\n",
        "ROLE_NAMES = [\n",
        "    VILLAGER,\n",
        "    WEREWOLF,\n",
        "    SEER,\n",
        "]\n",
        "ROLE_TYPE = Literal[tuple(ROLE_NAMES)]\n",
        "\n",
        "UTTERANCES = 10\n",
        "TABLE_CARDS = 2\n",
        "ROLE_COUNTS = {\n",
        "    VILLAGER: 7,\n",
        "    WEREWOLF: 3,\n",
        "    SEER: 2,\n",
        "}\n",
        "ALLOW_SEER_VOTE_SELF = False\n",
        "ALLOW_WEREWOLVES_VOTE_WEREWOLF = False\n",
        "ALLOW_LYNCH_VOTE_SELF = False\n",
        "\n",
        "PLAYER_NAMES = [\"Alice\", \"Bob\", \"Jim\", \"Friderik\", \"Janez\", \"Doris\", \"Paella\", \"Kiki\", \"Chiko\", \"Lima\", \"Romeo\", \"Juliet\"]\n",
        "SYSTEM = \"System\"\n",
        "NAME_TYPE = Literal[tuple(PLAYER_NAMES)]\n",
        "\n",
        "if sum(ROLE_COUNTS.values()) - TABLE_CARDS > len(PLAYER_NAMES):\n",
        "    raise RuntimeError"
      ],
      "metadata": {
        "id": "gHc5AtuHGPtT",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.264019Z"
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Message(BaseModel):\n",
        "    name: NAME_TYPE | Literal[SYSTEM]\n",
        "    message: str"
      ],
      "metadata": {
        "id": "uTin0KSRHPF2",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.265076Z"
        }
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The player class\n",
        "\n",
        "Now, let us define the player class. Broadly speaking the player can `vote` and `speak` at different points in the game. If the player gets mauled or lynched, they are dead (and thus silent) for the rest of the game."
      ],
      "metadata": {
        "id": "xq3iglWqNmks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Player:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: NAME_TYPE,\n",
        "        role: ROLE_TYPE,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.visibility = {}\n",
        "        self.history: list[Message] = []\n",
        "        self.instructions_suffix = \"\"\n",
        "\n",
        "        self.alive = True\n",
        "\n",
        "    def build_instructions(self):\n",
        "        instructions = f\"\"\"\n",
        "You are playing the game werewolf with several other LLMs.\n",
        "The goal of the werewolves is to eat the villagers at night without being caught during the day.\n",
        "The goal of the villagers is to figure out who the werewolves are during the day by talking, and then voting them out of the village.\n",
        "The villagers win if they vote all the werewolves out.\n",
        "The werewolves win if they get majority.\n",
        "There are {ROLE_COUNTS[VILLAGER]} villagers, {ROLE_COUNTS[WEREWOLF]} werewolves, {ROLE_COUNTS[SEER]} seers, and {TABLE_CARDS} roles that are face-down on the table.\n",
        "\n",
        "Your name is {self.name}, your role is {self.role}.]\n",
        "\"\"\"\n",
        "        formatted_visibility = '\\n'.join(f' - {name}: {role}' for name, role in self.visibility.items())\n",
        "        if formatted_visibility:\n",
        "            instructions += \"You know the following roles:\\n\" + formatted_visibility\n",
        "        if self.instructions_suffix:\n",
        "            instructions += f'\\n{self.instructions_suffix}'\n",
        "\n",
        "        return instructions\n",
        "\n",
        "    def speak(self):\n",
        "        instructions_stem = f\"BE SHORT AND SUCCINCT, REPLY ONLY WITH {self.name}'s RESPONSE\"\n",
        "        instructions = self.build_instructions() + '\\n' + instructions_stem\n",
        "\n",
        "        formatted_history = \"\\n\\n\".join(\n",
        "            f\"{msg.name}: {msg.message}\"\n",
        "            for msg in self.history\n",
        "        )\n",
        "        prompt = f\"\"\"History:\n",
        "```\n",
        "{formatted_history}\n",
        "```\n",
        "\n",
        "{instructions_stem}\n",
        "\n",
        "EXAMPLE_RESPONSE: `Let's figure out who the werewolf is!`\n",
        "\n",
        "As {self.name} who is a {self.role} I say:\n",
        "\"\"\"\n",
        "\n",
        "        response = prompt_with_instructions(instructions, prompt)\n",
        "        return response\n",
        "\n",
        "    def vote(\n",
        "        self,\n",
        "        player_names: list[NAME_TYPE],\n",
        "        prompt_suffix: str,\n",
        "    ):\n",
        "        instructions = self.build_instructions()\n",
        "\n",
        "        formatted_history = \"\\n\\n\".join(\n",
        "            f\"{msg.name}: {msg.message}\"\n",
        "            for msg in self.history\n",
        "        )\n",
        "        prompt = f\"\"\"History:\n",
        "```\n",
        "{formatted_history}\n",
        "```\n",
        "\n",
        "{prompt_suffix}\n",
        "\"\"\"\n",
        "\n",
        "        result = choose(instructions, prompt, player_names)\n",
        "        if result is None:\n",
        "            print(f\"{self.name} didn't vote properly\")\n",
        "        return result\n",
        "\n",
        "    def kill(self):\n",
        "        self.alive = False"
      ],
      "metadata": {
        "id": "64JR0GUEHPzA",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.265906Z"
        }
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The round\n",
        "\n",
        "Now let us define the game itself. It consists of Rounds owned by the Round class.\n",
        "\n",
        "A round supports the `prepare` and `run` commands."
      ],
      "metadata": {
        "id": "sQuqcEHKN7J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Round:\n",
        "    def __init__(\n",
        "        self,\n",
        "        players: list[Player],\n",
        "        table: list[ROLE_TYPE],\n",
        "    ):\n",
        "        self.all_players = players\n",
        "        self.table = table\n",
        "        self.conversation_history: list[Message] = []\n",
        "        self.roles: dict[NAME_TYPE, ROLE_TYPE] = {\n",
        "            p.name: p.role\n",
        "            for p in players\n",
        "        }\n",
        "        self.players_by_name = {\n",
        "            p.name: p\n",
        "            for p in players\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def players(self):\n",
        "        return [p for p in self.all_players if p.alive]\n",
        "\n",
        "    def prepare(self, player: Player):\n",
        "        if player.role == VILLAGER:\n",
        "            player.instructions_suffix = f\"Figure out who the werewolf is, then vote for them\"\n",
        "        elif player.role == WEREWOLF:\n",
        "            player.visibility |= {\n",
        "                player.name: WEREWOLF\n",
        "                for player in self.all_players\n",
        "                if player.role == WEREWOLF\n",
        "            }\n",
        "        elif player.role == SEER:\n",
        "            # TODO let them choose instead of random\n",
        "            choices = [\n",
        "                player.name for player in self.all_players\n",
        "            ]\n",
        "            choices += [\n",
        "                f\"Table card {i}\" for i in range(len(self.table))\n",
        "            ]\n",
        "            choice = random.choice(choices)\n",
        "            if choice.startswith(\"Table card \"):\n",
        "                i = int(choice.removeprefix(\"Table card \"))\n",
        "                # table\n",
        "                player.visibility |= {\n",
        "                    choice: self.table[i]\n",
        "                }\n",
        "            else:\n",
        "                chosen_player = self.players_by_name[choice]\n",
        "                player.visibility |= {\n",
        "                    choice: chosen_player.role\n",
        "                }\n",
        "        else:\n",
        "            assert_never(player)\n",
        "            raise RuntimeError\n",
        "\n",
        "    def act_at_night(self, role: ROLE_TYPE) -> Optional[str]:\n",
        "        if role == VILLAGER:\n",
        "            raise RuntimeError\n",
        "        elif role == WEREWOLF:\n",
        "            werewolves = [\n",
        "                player for player in self.players\n",
        "                if player.role == WEREWOLF\n",
        "            ]\n",
        "            votes = defaultdict(int)\n",
        "\n",
        "            print(f\"\\n-------------- \\n\")\n",
        "\n",
        "            print(\"The werewolves have awoken\")\n",
        "            for werewolf in werewolves:\n",
        "                if not ALLOW_WEREWOLVES_VOTE_WEREWOLF:\n",
        "                    choices = [\n",
        "                        p.name for p in self.players\n",
        "                        if p.role != WEREWOLF\n",
        "                    ]\n",
        "                else:\n",
        "                    choices = [p.name for p in self.players]\n",
        "                chosen_player = werewolf.vote(\n",
        "                    choices,\n",
        "                    f\"It's night-time. As a werewolf, you must vote for who to kill. Who will you vote for, {werewolf.name}?\"\n",
        "                )\n",
        "                if chosen_player is None:\n",
        "                    continue\n",
        "                print(f\"{werewolf.name} voted for {chosen_player}\")\n",
        "                votes[chosen_player] += 1\n",
        "            max_votes = max(votes.values())\n",
        "            tie_players = [player for player, num_votes in votes.items() if num_votes == max_votes]\n",
        "\n",
        "            if len(tie_players) > 1:\n",
        "                print(\"The werewolves tied between the following players:\", tie_players)\n",
        "            else:\n",
        "                print(\"They voted for:\", tie_players[0])\n",
        "            chosen_player = tie_players[0]\n",
        "\n",
        "            player = self.players_by_name[chosen_player]\n",
        "            player.kill()\n",
        "            return f\"{player.name} was found mauled in their own home. Must have been the werewolves!\"\n",
        "        elif role == SEER:\n",
        "            choices = [\n",
        "                player.name for player in self.all_players\n",
        "            ]\n",
        "            choices += [\n",
        "                f\"Table role {i}\" for i in range(len(self.table))\n",
        "            ]\n",
        "            seers = [\n",
        "                player for player in self.players\n",
        "                if player.role == SEER\n",
        "            ]\n",
        "            for seer in seers:\n",
        "                if not ALLOW_SEER_VOTE_SELF:\n",
        "                    seer_choices = list(\n",
        "                        set(choices) - {seer.name}\n",
        "                    )\n",
        "                else:\n",
        "                    seer_choices = choices\n",
        "                choice = seer.vote(\n",
        "                    seer_choices,\n",
        "                    f\"It's night-time. As a seer, you may look at one of the player's roles, or one of the roles on the table face-down. Which one will you look at, {seer.name}?\"\n",
        "                )\n",
        "                if choice is None:\n",
        "                    continue\n",
        "                if choice.startswith(\"Table role \"):\n",
        "                    i = int(choice.removeprefix(\"Table role \"))\n",
        "                    result = self.table[i]\n",
        "                else:\n",
        "                    result = self.players_by_name[choice].role\n",
        "                seer.visibility |= {\n",
        "                    choice: result\n",
        "                }\n",
        "                msg = f\"As the seer, you have chosen to see {choice}, you saw {result}\"\n",
        "                print(f\"{seer.name}: {msg}\")\n",
        "                seer.history.append(\n",
        "                    Message(\n",
        "                        name=SYSTEM,\n",
        "                        message=msg,\n",
        "                    )\n",
        "                )\n",
        "            return None\n",
        "        else:\n",
        "            assert_never(role)\n",
        "            raise RuntimeError\n",
        "\n",
        "    def run(self):\n",
        "        print(\"Roles:\")\n",
        "        for player in self.all_players:\n",
        "            print(f\"  - {player.name}: {player.role}\")\n",
        "\n",
        "        # Setup (giving info to players on who is who)\n",
        "        for player in self.all_players:\n",
        "            self.prepare(player)\n",
        "\n",
        "        round_num = 0\n",
        "        while round_num < 10:\n",
        "            # Night phase\n",
        "            results = []\n",
        "            for role in [WEREWOLF, SEER]:\n",
        "                if role == WEREWOLF and round_num == 0:\n",
        "                    continue\n",
        "                result = self.act_at_night(role)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "            for result in results:\n",
        "                message = Message(\n",
        "                    name=SYSTEM,\n",
        "                    message=result,\n",
        "                )\n",
        "                print(result)\n",
        "                for player in self.players:\n",
        "                    player.history.append(message)\n",
        "\n",
        "            # Discussion phase\n",
        "            last_speaker = None\n",
        "            for _ in range(UTTERANCES):\n",
        "                available_players = set(self.players)\n",
        "                if last_speaker is not None:\n",
        "                    available_players -= {last_speaker}\n",
        "                chosen_player = random.choice(list(available_players))\n",
        "\n",
        "                response = chosen_player.speak()\n",
        "\n",
        "                print(f\"\\n[{chosen_player.name}]:\\n {response}\\n\")\n",
        "                print(f\"-------------- \\n\")\n",
        "\n",
        "                message = Message(\n",
        "                    name=chosen_player.name,\n",
        "                    message=response,\n",
        "                )\n",
        "                for player in self.players:\n",
        "                    player.history.append(message)\n",
        "\n",
        "                last_speaker = chosen_player\n",
        "\n",
        "            # Voting phase\n",
        "            votes = defaultdict(int)\n",
        "            vote_messages = []\n",
        "            print(\"Time to lynch\")\n",
        "            for player in self.players:\n",
        "                choices = [p.name for p in self.players]\n",
        "                if not ALLOW_LYNCH_VOTE_SELF:\n",
        "                    choices = list(\n",
        "                        set(choices) - {player.name}\n",
        "                    )\n",
        "                voted_player = player.vote(\n",
        "                    [\n",
        "                        p.name for p in self.players\n",
        "                        if p is not player\n",
        "                    ],\n",
        "                    f\"Discussions are now closed. Who are you going to vote for to kill, {player.name}?\"\n",
        "                )\n",
        "                if voted_player is None:\n",
        "                    continue\n",
        "\n",
        "                msg = f\"{player.name} voted for {voted_player}\"\n",
        "                print(msg)\n",
        "                message = Message(\n",
        "                    name=SYSTEM,\n",
        "                    message=msg,\n",
        "                )\n",
        "                vote_messages.append(message)\n",
        "\n",
        "                votes[voted_player] += 1\n",
        "\n",
        "            for message in vote_messages:\n",
        "                for player in self.players:\n",
        "                    player.history.append(message)\n",
        "\n",
        "            max_votes = max(votes.values())\n",
        "            tie_players = [player for player, num_votes in votes.items() if num_votes == max_votes]\n",
        "\n",
        "            if len(tie_players) > 1:\n",
        "                msg = f\"No one died, because there was a tie between the following players: {','.join(tie_players)}\"\n",
        "            else:\n",
        "                msg = f\"The people voted for {tie_players[0]}, they have been lynched. RIP\"\n",
        "                self.players_by_name[tie_players[0]].kill()\n",
        "\n",
        "            # Check win condition\n",
        "            werewolf_count = len([\n",
        "                player for player in self.players\n",
        "                if player.role == WEREWOLF\n",
        "            ])\n",
        "            if werewolf_count == 0:\n",
        "                print(\"VILLAGERS WIN!\")\n",
        "                break\n",
        "            if werewolf_count > len(self.players) // 2 or len(self.players) == werewolf_count:\n",
        "                print(\"WEREWOLVES WIN!\")\n",
        "                break\n",
        "\n",
        "            print(msg)\n",
        "            message = Message(\n",
        "                name=SYSTEM,\n",
        "                message=msg,\n",
        "            )\n",
        "            for player in self.players:\n",
        "                player.history.append(message)\n",
        "\n",
        "            round_num += 1"
      ],
      "metadata": {
        "id": "ygjeA7SeHc1M",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.267153Z"
        }
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the round\n",
        "\n",
        "In here we hand out cards to players to give them roles and define our game."
      ],
      "metadata": {
        "id": "wk5QAacyOBHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "players = []\n",
        "names = PLAYER_NAMES[:]\n",
        "roles = dict(ROLE_COUNTS)\n",
        "\n",
        "table_cards = []\n",
        "# pick out table cards\n",
        "for _ in range(TABLE_CARDS):\n",
        "    available_roles = [\n",
        "        role for role, count in roles.items()\n",
        "        if count > 1\n",
        "    ]\n",
        "    picked_role = random.choice(available_roles)\n",
        "    roles[picked_role] -= 1\n",
        "    table_cards.append(picked_role)\n",
        "\n",
        "# assign roles\n",
        "for role, count in roles.items():\n",
        "    for _ in range(count):\n",
        "        name = random.choice(names)\n",
        "        names.remove(name)\n",
        "        players.append(\n",
        "            Player(\n",
        "                name=name,\n",
        "                role=role,\n",
        "            )\n",
        "        )\n",
        "\n",
        "round = Round(\n",
        "    players=players,\n",
        "    table=table_cards,\n",
        ")"
      ],
      "metadata": {
        "id": "oqJpo2L6Hn9d",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.268134Z"
        }
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter Llama output (optional)\n",
        "\n",
        "Earlier, we set verbose=True in our LlamaCPP model. Unfortunately, we cannot remove this due to a bug. So we will filter stderr to remove Llama output, and focus on the game."
      ],
      "metadata": {
        "id": "kLnIK1nuRu4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "class Filter(object):\n",
        "    def __init__(self, stream, re_pattern):\n",
        "        self.stream = stream\n",
        "        self.pattern = re.compile(re_pattern) if isinstance(re_pattern, str) else re_pattern\n",
        "        self.triggered = False\n",
        "\n",
        "    def __getattr__(self, attr_name):\n",
        "        return getattr(self.stream, attr_name)\n",
        "\n",
        "    def write(self, data):\n",
        "        if data == '\\n' and self.triggered:\n",
        "            self.triggered = False\n",
        "        else:\n",
        "            if self.pattern.search(data) is None:\n",
        "                self.stream.write(data)\n",
        "                self.stream.flush()\n",
        "            else:\n",
        "                # caught bad pattern\n",
        "                self.triggered = True\n",
        "\n",
        "    def flush(self):\n",
        "        self.stream.flush()\n",
        "\n",
        "# example\n",
        "sys.stderr = Filter(sys.stdout, r'Llama.generate: prefix-match hit')  # filter out any line which contains \"Read -1\" in it\n"
      ],
      "metadata": {
        "id": "PuRF00z7RdUb",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.269007Z"
        }
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the game\n",
        "\n",
        "Finally, let's unleash the werewolves!"
      ],
      "metadata": {
        "id": "tovkmF_7OEMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##%%\n",
        "round.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ByQtar1HriZ",
        "outputId": "3b1ef7ce-a9dc-4eee-8c1e-4ce72d4b2536",
        "ExecuteTime": {
          "start_time": "2023-11-16T19:47:08.269980Z"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roles:\n",
            "  - Chiko: Villager\n",
            "  - Friderik: Villager\n",
            "  - Alice: Villager\n",
            "  - Juliet: Villager\n",
            "  - Lima: Villager\n",
            "  - Romeo: Villager\n",
            "  - Janez: Villager\n",
            "  - Paella: Werewolf\n",
            "  - Jim: Werewolf\n",
            "  - Bob: Seer\n",
            "Bob: As the seer, you have chosen to see Table role 0, you saw Werewolf\n",
            "\n",
            "[Jim]:\n",
            " I agree, we need to be vigilant and find out who these werewolves are!\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Bob]:\n",
            " I had a vision last night; we definitely have a werewolf among the face-down roles. Let's be careful with our accusations and think this through.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Janez]:\n",
            " We should listen to the seers and consider the face-down roles carefully while discussing.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Juliet]:\n",
            " We should cross-reference Bob's vision with other clues and be strategic about our questioning.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Paella]:\n",
            " We should definitely listen to Bob, but let's not jump to conclusions. Everyone's input is vital.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Lima]:\n",
            " Has anyone else, maybe the other seer, had a vision that can confirm or contradict Bob's statement about the face-down roles?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Janez]:\n",
            " Has the other seer received any insights that can support or refute Bob's claim?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Jim]:\n",
            " Absolutely, we should hear from the other seer to get a clearer picture.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Lima]:\n",
            " If the other seer could share their vision, we might be able to piece this together better. Any clues?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Paella]:\n",
            " We should definitely hear from the other seer. It's important for us to cross-validate the information to make an informed decision.\n",
            "\n",
            "-------------- \n",
            "\n",
            "Time to lynch\n",
            "Chiko voted for Jim\n",
            "Friderik voted for Bob\n",
            "Alice voted for Janez\n",
            "Juliet voted for Jim\n",
            "Lima voted for Bob\n",
            "Romeo voted for Janez\n",
            "Janez voted for Jim\n",
            "Paella voted for Bob\n",
            "Jim voted for Juliet\n",
            "Bob voted for Jim\n",
            "The people voted for Jim, they have been lynched. RIP\n",
            "\n",
            "-------------- \n",
            "\n",
            "The werewolves have awoken\n",
            "Paella voted for Chiko\n",
            "They voted for: Chiko\n",
            "Bob: As the seer, you have chosen to see Table role 1, you saw Seer\n",
            "Chiko was found mauled in their own home. Must have been the werewolves!\n",
            "\n",
            "[Lima]:\n",
            " Bob's vision doesn't seem to help now. We need the other seer's input, or any other clues. Who defended Jim or voted differently?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Bob]:\n",
            " I've confirmed the second face-down role is a Seer, not a threat. We should focus on players‚Äô behaviors and voting patterns from yesterday for clues.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Janez]:\n",
            " We need to consider voting patterns and any defenses made for Jim. A werewolf would likely try to save another.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Lima]:\n",
            " Reviewing yesterday's votes, it's unusual that Friderik and Romeo voted without solid reasoning. Could they have been trying to deflect suspicion away from the real werewolf?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Alice]:\n",
            " I think Lima makes a good point about Friderik and Romeo's votes. Can they explain their reasoning?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Romeo]:\n",
            " My vote for Janez was a hunch, nothing more. There was no solid evidence. We should definitely focus on behavioral patterns and possible defenses of Jim before his lynching to find clues pointing to the werewolves.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Juliet]:\n",
            " Bob's vision of the face-down roles helps, but I noticed Friderik and Romeo's votes seemed random. Do they have explanations that can allay our suspicions?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Romeo]:\n",
            " I realize my previous vote seemed random; it was an instinctive choice due to indecision. Let's look closer at who defended Jim and who was quick to diverge the votes, as it might indicate werewolf behavior.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Juliet]:\n",
            " Friderik and Romeo's votes were indeed strange. Can either of you provide more insight on your choice?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Friderik]:\n",
            " My vote for Bob was based on his vision claim, which I was unsure about. In hindsight, it might've been a mistake. Let's focus on behavioral patterns and look for inconsistencies in statements and votes.\n",
            "\n",
            "-------------- \n",
            "\n",
            "Time to lynch\n",
            "Friderik voted for Bob\n",
            "Alice voted for Friderik\n",
            "Juliet voted for Friderik\n",
            "Lima voted for Friderik\n",
            "Romeo voted for Friderik\n",
            "Janez voted for Friderik\n",
            "Paella voted for Alice\n",
            "Bob voted for Friderik\n",
            "The people voted for Friderik, they have been lynched. RIP\n",
            "\n",
            "-------------- \n",
            "\n",
            "The werewolves have awoken\n",
            "Paella voted for Alice\n",
            "They voted for: Alice\n",
            "Bob: As the seer, you have chosen to see Janez, you saw Villager\n",
            "Alice was found mauled in their own home. Must have been the werewolves!\n",
            "\n",
            "[Juliet]:\n",
            " Friderik's defense didn't convince me, and now Alice is gone too. We need to look at who's been avoiding suspicion.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Bob]:\n",
            " Janez is a villager. We must now focus on those who've cast suspicious votes and the defenses that have been made so far.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Lima]:\n",
            " Paella's vote against Alice stands out; it doesn't align with the majority who suspected Friderik. Could Paella be trying to protect another werewolf?\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Romeo]:\n",
            " Paella's vote for Alice was indeed suspicious, as it didn't follow the consensus on Friderik. We should question Paella's motives for that decision.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Paella]:\n",
            " I understand why my vote might seem odd, but I genuinely thought Alice's behavior was suspicious and didn't completely trust the case against Friderik. We should keep scrutinizing everyone's behavior, including those who are quick to cast doubt without much evidence\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Romeo]:\n",
            " Paella's rogue vote for Alice could be an attempt to misdirect us. We should press Paella for a better explanation or consider them a suspect.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Janez]:\n",
            " Bob confirmed I'm a villager. Paella's lone vote against Alice was suspicious. Pressing Paella for a clearer explanation or considering them a suspect seems reasonable.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Juliet]:\n",
            " Paella's vote against Alice does stand out. We need a better explanation or we might have to consider it a sign of werewolf behavior.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Bob]:\n",
            " Paella's solitary vote against Alice is concerning. It's critical we understand that choice to determine if it was a genuine suspicion or a werewolf tactic.\n",
            "\n",
            "-------------- \n",
            "\n",
            "\n",
            "[Janez]:\n",
            " Press Paella for a clear explanation of their vote against Alice.\n",
            "\n",
            "-------------- \n",
            "\n",
            "Time to lynch\n",
            "Juliet voted for Paella\n",
            "Lima voted for Paella\n",
            "Romeo voted for Paella\n",
            "Janez voted for Paella\n",
            "Paella voted for Bob\n",
            "Bob voted for Paella\n",
            "VILLAGERS WIN!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final thoughts\n",
        "\n",
        "Who won? The villagers? The Werewolves? Let us know!\n"
      ],
      "metadata": {
        "id": "9SGJivep2Icz"
      }
    }
  ]
}
